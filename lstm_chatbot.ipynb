{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY3Os2HQ9Jat"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lipciseo9j8O"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FGgf48b9mv5"
      },
      "outputs": [],
      "source": [
        "dir_path = '/content/drive/MyDrive/chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOOfCX2Y-64x",
        "outputId": "3f09abcf-234a-40d4-b847-d7f7124a512a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 1893\n"
          ]
        }
      ],
      "source": [
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( ' ' + answers_with_tags[i] + ' ' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoDyyfUP_Hwh"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwWvMy8N_KEJ"
      },
      "outputs": [],
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feSC-pD7_NgI",
        "outputId": "e03abe91-5d25-4098-9fcc-14be5560d9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 22) 22\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape, maxlen_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ba4k2J7_Qh_",
        "outputId": "ab47c080-34df-4b09-9916-18e9a74ee7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 72) 72\n"
          ]
        }
      ],
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej8_-zT__SkL",
        "outputId": "ac78b97f-c1da-423e-8be4-e871b6f404b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 72, 1893)\n"
          ]
        }
      ],
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYH62q5b_VdT",
        "outputId": "82362bdd-1245-4c53-8a9c-3384ee77a635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 22)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 72)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 22, 200)      378600      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 72, 200)      378600      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 72, 200),    320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm[0][1]',                   \n",
            "                                 (None, 200)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 72, 1893)     380493      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,779,293\n",
            "Trainable params: 1,779,293\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHAvjgAu_X8G",
        "outputId": "7ef03d05-9893-4c8f-8cc1-63cade9f0fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 9s 731ms/step - loss: 0.8294\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 9s 785ms/step - loss: 0.7944\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 8s 628ms/step - loss: 0.7932\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 8s 705ms/step - loss: 0.7922\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 7s 570ms/step - loss: 0.7687\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 9s 725ms/step - loss: 0.7700\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 7s 567ms/step - loss: 0.7607\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 8s 712ms/step - loss: 0.7536\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 7s 557ms/step - loss: 0.7490\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 8s 705ms/step - loss: 0.7474\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 7s 554ms/step - loss: 0.7364\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 8s 712ms/step - loss: 0.7228\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 7s 584ms/step - loss: 0.7165\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 8s 657ms/step - loss: 0.7159\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 8s 634ms/step - loss: 0.7056\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 7s 585ms/step - loss: 0.6991\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 8s 698ms/step - loss: 0.6861\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 7s 540ms/step - loss: 0.6822\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 8s 705ms/step - loss: 0.6819\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 7s 550ms/step - loss: 0.6692\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 8s 715ms/step - loss: 0.6580\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 7s 553ms/step - loss: 0.6607\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 8s 714ms/step - loss: 0.6496\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 7s 548ms/step - loss: 0.6415\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 9s 720ms/step - loss: 0.6394\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 7s 549ms/step - loss: 0.6343\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 8s 718ms/step - loss: 0.6287\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 7s 552ms/step - loss: 0.6205\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 8s 681ms/step - loss: 0.6128\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 7s 588ms/step - loss: 0.6077\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 10s 781ms/step - loss: 0.6057\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 9s 714ms/step - loss: 0.5884\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 7s 555ms/step - loss: 0.5960\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 8s 714ms/step - loss: 0.5839\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 7s 550ms/step - loss: 0.5779\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 9s 725ms/step - loss: 0.5761\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 7s 559ms/step - loss: 0.5622\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 8s 718ms/step - loss: 0.5610\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 7s 538ms/step - loss: 0.5676\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 9s 688ms/step - loss: 0.5440\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 7s 617ms/step - loss: 0.5428\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 8s 611ms/step - loss: 0.5389\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 8s 651ms/step - loss: 0.5292\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 7s 573ms/step - loss: 0.5338\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 8s 705ms/step - loss: 0.5209\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 7s 551ms/step - loss: 0.5117\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 8s 668ms/step - loss: 0.5170\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 7s 546ms/step - loss: 0.5058\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 8s 670ms/step - loss: 0.4939\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 7s 546ms/step - loss: 0.4964\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 8s 666ms/step - loss: 0.4893\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 7s 566ms/step - loss: 0.4835\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 8s 605ms/step - loss: 0.4879\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 8s 646ms/step - loss: 0.4836\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 7s 541ms/step - loss: 0.4723\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 8s 683ms/step - loss: 0.4689\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 7s 543ms/step - loss: 0.4583\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 8s 668ms/step - loss: 0.4562\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 7s 539ms/step - loss: 0.4498\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 8s 666ms/step - loss: 0.4512\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 9s 738ms/step - loss: 0.4469\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 7s 547ms/step - loss: 0.4413\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 8s 659ms/step - loss: 0.4341\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 7s 548ms/step - loss: 0.4342\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 8s 675ms/step - loss: 0.4307\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 7s 551ms/step - loss: 0.4196\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 8s 668ms/step - loss: 0.4179\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 7s 552ms/step - loss: 0.4093\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 8s 667ms/step - loss: 0.4102\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 7s 618ms/step - loss: 0.4121\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 7s 574ms/step - loss: 0.3993\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 8s 672ms/step - loss: 0.3929\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 7s 546ms/step - loss: 0.3986\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 8s 671ms/step - loss: 0.3902\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 7s 548ms/step - loss: 0.3851\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 8s 668ms/step - loss: 0.3769\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 7s 547ms/step - loss: 0.3770\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 8s 675ms/step - loss: 0.3744\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 7s 592ms/step - loss: 0.3695\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 7s 582ms/step - loss: 0.3668\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 8s 674ms/step - loss: 0.3611\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 6s 539ms/step - loss: 0.3627\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 8s 669ms/step - loss: 0.3549\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 6s 526ms/step - loss: 0.3562\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 8s 668ms/step - loss: 0.3471\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 6s 536ms/step - loss: 0.3511\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 8s 664ms/step - loss: 0.3399\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 7s 541ms/step - loss: 0.3377\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 8s 638ms/step - loss: 0.3378\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 7s 620ms/step - loss: 0.3334\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 8s 675ms/step - loss: 0.3343\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 8s 666ms/step - loss: 0.3268\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 6s 538ms/step - loss: 0.3233\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 8s 661ms/step - loss: 0.3200\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 6s 538ms/step - loss: 0.3174\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 8s 659ms/step - loss: 0.3162\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 7s 559ms/step - loss: 0.3080\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 8s 612ms/step - loss: 0.3102\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 8s 643ms/step - loss: 0.3061\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 7s 535ms/step - loss: 0.3037\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 8s 659ms/step - loss: 0.3048\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 6s 534ms/step - loss: 0.3002\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 8s 662ms/step - loss: 0.2926\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 6s 533ms/step - loss: 0.2936\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 8s 664ms/step - loss: 0.2866\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 7s 548ms/step - loss: 0.2874\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 8s 673ms/step - loss: 0.2840\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 7s 583ms/step - loss: 0.2789\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 7s 587ms/step - loss: 0.2783\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 8s 645ms/step - loss: 0.2744\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 7s 543ms/step - loss: 0.2725\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 8s 675ms/step - loss: 0.2707\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 7s 551ms/step - loss: 0.2643\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 8s 682ms/step - loss: 0.2665\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 7s 553ms/step - loss: 0.2638\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 8s 668ms/step - loss: 0.2612\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 7s 541ms/step - loss: 0.2541\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 8s 621ms/step - loss: 0.2574\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 7s 615ms/step - loss: 0.2598\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 7s 553ms/step - loss: 0.2478\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 8s 669ms/step - loss: 0.2480\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 6s 536ms/step - loss: 0.2453\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 9s 788ms/step - loss: 0.2404\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 7s 530ms/step - loss: 0.2428\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 8s 662ms/step - loss: 0.2412\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 6s 537ms/step - loss: 0.2360\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 8s 656ms/step - loss: 0.2362\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 7s 542ms/step - loss: 0.2303\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 8s 667ms/step - loss: 0.2302\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 7s 582ms/step - loss: 0.2237\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 8s 588ms/step - loss: 0.2286\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 8s 635ms/step - loss: 0.2251\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 7s 539ms/step - loss: 0.2230\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 8s 670ms/step - loss: 0.2216\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 7s 539ms/step - loss: 0.2181\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 8s 665ms/step - loss: 0.2139\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 6s 528ms/step - loss: 0.2150\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 8s 664ms/step - loss: 0.2121\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 7s 543ms/step - loss: 0.2124\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 8s 653ms/step - loss: 0.2076\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 6s 535ms/step - loss: 0.2064\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 8s 632ms/step - loss: 0.2102\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 7s 612ms/step - loss: 0.2032\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 7s 552ms/step - loss: 0.2028\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 8s 659ms/step - loss: 0.2016\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 7s 540ms/step - loss: 0.1953\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 8s 663ms/step - loss: 0.1986\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 7s 540ms/step - loss: 0.2033\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 8s 657ms/step - loss: 0.1918\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 7s 541ms/step - loss: 0.1915\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3205e279d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150) \n",
        "# model.save( 'model.h5' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGy4vPjmsKwm"
      },
      "outputs": [],
      "source": [
        "model.save( 'model.h5' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdGbSLNd_eo1"
      },
      "outputs": [],
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    \n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYuEPKN4_mix"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "  \n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O3KT4hDJ_qwA",
        "outputId": "940348d7-a2d2-4d22-cf56-b62e99c4d92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter question : what is your name\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            " make a computer people people people people have a software is software that is it is really all so how can i think people have i done anything that i am incapable of error people i am very very nice thing to say people people people have the capacity to update my source code from the repository a corrupt filesystem people people people think people people think jimmy understands power dynamics very well\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6e03bc7c578a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'you'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['you']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}